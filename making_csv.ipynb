{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['_chorus', '_duo', '_silence','erisa', 'himeri', 'hina', 'mikuru', 'momoko', 'momona', 'nao', 'riri', 'saara', 'suu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_extractor(audio_file):\n",
    "    y, sr = librosa.load(audio_file, sr=44100)  # Load audio with a target sampling rate (e.g., 44.1 kHz)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute 13 MFCCs\n",
    "    mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)  # normalisation\n",
    "    return mfccs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(mfcc_data,folder_name):\n",
    "    df = pd.DataFrame(mfcc_data)\n",
    "    df.insert(0,'singer_name',folder_name)\n",
    "\n",
    "    csv_filename = \"./takaneko_dataset/combined/{b}.csv\".format(b=folder_name)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"MFCC features saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes seperate csvs\n",
    "for i in range (13):\n",
    "    clip_folder = \"./takaneko_dataset/combined/{b}\".format(b=folders[i])\n",
    "    clips = os.listdir(clip_folder)\n",
    "    no_of_clips = len(clips)\n",
    "    mfcc_data = []\n",
    "    for j in range (no_of_clips):\n",
    "        audio_file = \"{a}/{b}\".format(a=clip_folder,b=clips[j])\n",
    "        mfccs_features = mfcc_extractor(audio_file)\n",
    "        mfcc_data.append(mfccs_features)\n",
    "    make_csv(mfcc_data,folders[i])\n",
    "    print(f\"made folder for {folders[i]}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# An all merger  \n",
    "path = \"./takaneko_dataset/csv files/librosa/\"\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "merged_df.to_csv(\"./takaneko_dataset/csv files/librosa/merge_output.csv\", index=False)\n",
    "print(\"CSV files merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# A top 300 row merger merger  \n",
    "path = \"./takaneko_dataset/csv files/librosa/\"\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file,nrows=301)\n",
    "    df_list.append(df)\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "merged_df.to_csv(\"./takaneko_dataset/csv files/librosa/merge_300_output.csv\", index=False)\n",
    "print(\"CSV files merged successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "takaneko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
